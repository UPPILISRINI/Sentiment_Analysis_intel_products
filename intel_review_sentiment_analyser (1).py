# -*- coding: utf-8 -*-
"""Intel_Review_sentiment_analyser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RnJw4oSp9P15BTUV3J6_lMMP-ZJk3-Xk
"""

import pandas as pd
import nltk

nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

!pip install vaderSentiment

data=pd.read_csv('/content/Full_Intel_Product_Reviews.csv')
data.head(10)

data=data.drop(['country','reviewTitle'],axis=1)
data.head(10)

data.describe()

missing_values = data.isnull().sum()
print(missing_values)

duplicate_rows = data.duplicated()
duplicate_rows

import re

def clean_text(text):
   # remove URLs
  text = re.sub(r'http\S+', '', text)

  # remove special characters and punctuation
  text = re.sub(r'[^a-zA-Z\s]', '', text)
  text = re.sub(r'<.*?>', '', text)
  text = re.sub(r"n't", " not", text)
  text = re.sub(r"'re", " are", text)
  text = re.sub(r"'s", " is", text)
  text = re.sub(r"'d", " would", text)
  text = re.sub(r"'ll", " will", text)
  text = re.sub(r"'t", " not", text)
  text = re.sub(r"'ve", " have", text)
  text = re.sub(r"'m", " am", text)
  text = text.lower()
  text = re.sub(r'[^\w\s]', '', text)

  # convert text to lowercase
  text = text.lower()

  # tokenize the text
  text = nltk.word_tokenize(text)

  # remove stopwords
  text = [word for word in text if word not in nltk.corpus.stopwords.words('english')]

  # remove short words
  text = [word for word in text if len(word) > 2]

  # lemmatize the words
  lemmatizer = nltk.stem.WordNetLemmatizer()
  text = [lemmatizer.lemmatize(word) for word in text]
  return ' '.join(text)

data['reviewDescription'] = data['reviewDescription'].apply(clean_text)

data.head(10)

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

analyzer = SentimentIntensityAnalyzer()

data['compound_score'] = data['reviewDescription'].apply(lambda x: analyzer.polarity_scores(x)['compound'])

data.head(50)

def get_sentiment(score):
  if score >= 0.05:
    return 'positive'
  elif score <= -0.05:
    return 'negative'
  else:
    return 'neutral'

data['Sentiment'] = data['compound_score'].apply(get_sentiment)

data.head(50)

sentiment_counts = data['Sentiment'].value_counts()
percentage = sentiment_counts / sentiment_counts.sum() * 100
print(percentage)

processors=data['variant'].unique()
for i in processors:
  print(i)
len(processors)

import numpy as np

def reduce_dataset(data):
  for processor in data['variant'].unique():
    processor_data = data[data['variant'] == processor]
    num_rows = len(processor_data)
    sample_size = int(num_rows / 2)
    data = data.drop(processor_data.sample(num_rows - sample_size).index)
  return data

data = reduce_dataset(data)

wanted_review=['Intel-core-i5-12000','Intel-core-i9-12000','Intel-core-i7-12000','Intel-core-i5-13000','Intel-core-i7-13000','Intel-core-i9-13000','Intel-core-i9-14000','Intel-core-i7-14000','Intel-core-i5-14000']

separated_reviews = {}
for variant in wanted_review:
  separated_reviews[variant] = data[data['variant'] == variant]

technical_keywords = [
    'x86', 'x64', 'ARM', 'RISC', 'CISC', 'GHz', 'Clock speed', 'Core count',
    'Thread count', 'Benchmark', 'IPC', 'Overclock', 'Turbo Boost', 'Hyper-Threading',
    'Thermal Design Power', 'TDP', 'Cache', 'L1', 'L2', 'L3', 'Intel Core',
    'Simultaneous Multithreading', 'SMT', 'Integrated Graphics', 'iGPU',
    'Thermal throttling', 'Die shrink', 'Lithography', 'PCIe lanes',
    'Memory channels', 'ECC', 'Socket type', 'LGA', 'PGA', 'BGA', 'Chipset',
    'Motherboard compatibility', 'BIOS', 'UEFI', 'Cinebench', 'Geekbench',
    'PassMark', 'SPECint', 'SPECfp', 'Prime95', 'AIDA64', 'Cooling solutions',
    'Heatsink', 'Thermal paste', 'Voltage regulation', 'Gaming', 'Workstation',
    'Server', 'Embedded systems'
]

def classify_reviews(reviews, technical_keywords):
  classified_reviews = {}
  for variant, review_df in reviews.items():
    technical_reviews = []
    general_reviews = []
    for review in review_df['reviewDescription']:
      review_words = set(review.lower().split())
      if any(word in review_words for word in technical_keywords):
        technical_reviews.append(review)
      else:
        general_reviews.append(review)
    classified_reviews[variant] = {
      'technical': technical_reviews,
      'general': general_reviews
    }
  return classified_reviews

classified_reviews = classify_reviews(separated_reviews, technical_keywords)

general_review_lengths = {}
for variant, reviews in classified_reviews.items():
  general_review_lengths[variant] = len(reviews['general'])

for variant, length in general_review_lengths.items():
  print(f"Variant: {variant}, General Reviews: {length}")

total_general_reviews = sum(general_review_lengths.values())
print(f"Total General Reviews: {total_general_reviews}")

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

X = data['reviewDescription']
y = data['Sentiment']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create an SVM classifier
svm_classifier = SVC(kernel='linear')  # You can experiment with different kernels
vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)
# Train the classifier
svm_classifier.fit(X_train_vec, y_train)

# Make predictions on the test set
y_pred = svm_classifier.predict(X_test_vec)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')
conf_matrix = confusion_matrix(y_test, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1-score:", f1)
print("Confusion Matrix:\n", conf_matrix)

# create document term matrix

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(analyzer = 'word')

for variant, reviews in classified_reviews.items():
  general_reviews = reviews['general']
  dtm = vectorizer.fit_transform(general_reviews)
  data_dtm = pd.DataFrame(dtm.toarray(), columns=vectorizer.get_feature_names_out())
  # data.index = general_reviews.index
  print(f"Variant: {variant}")
  print(data_dtm.head(3))

import matplotlib.pyplot as plt

from wordcloud import WordCloud, STOPWORDS
import textwrap

# Function for generating word clouds
def generate_wordcloud(words, variant):
  wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=STOPWORDS).generate(words)
  plt.figure(figsize=(10, 5))
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis('off')
  plt.title(f'WordCloud for {variant} - General Reviews')
  plt.show()

for variant, reviews in classified_reviews.items():
  general_reviews = reviews['general']
  dtm = vectorizer.fit_transform(general_reviews)
  data_dtm = pd.DataFrame(dtm.toarray().T, index=vectorizer.get_feature_names_out(), columns=general_reviews)
  print(f"Variant: {variant}")
  print(data_dtm.head(3))

# Function for generating word clouds
def generate_wordcloud(words, variant):
  wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=STOPWORDS).generate(words)
  plt.figure(figsize=(10, 5))
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis('off')
  plt.title(f'WordCloud for {variant} - General Reviews')
  plt.show()

for variant, reviews in classified_reviews.items():
  general_reviews = reviews['general']
  all_words = ' '.join(general_reviews)
  generate_wordcloud(all_words, variant)
  print("\n")

#visualize the data reviews and sentiment for each variant
plt.figure(figsize=(10, 6))
for variant, reviews in classified_reviews.items():
  sentiment_counts = data['Sentiment'].value_counts()
  plt.bar(sentiment_counts.index, sentiment_counts.values)
  plt.title(f'Sentiment Analysis for {variant}')
  plt.xlabel('Sentiment')
  plt.ylabel('Count')
  plt.show()

import seaborn as sns

# Distribution of sentiments across variants
plt.figure(figsize=(12, 6))
for variant in wanted_review:
  variant_data = data[data['variant'] == variant]
  sentiment_counts = variant_data['Sentiment'].value_counts()
  plt.bar(sentiment_counts.index, sentiment_counts.values, label=variant)

plt.title('Distribution of Sentiments Across Variants')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.legend()
plt.show()

# Average compound score for each variant
average_scores = data.groupby('variant')['compound_score'].mean()
plt.figure(figsize=(12, 6))
plt.bar(average_scores.index, average_scores.values)
plt.title('Average Compound Score for Each Variant')
plt.xlabel('Variant')
plt.ylabel('Average Compound Score')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Word frequency analysis for each variant
top_words_per_variant = {}
for variant, reviews in classified_reviews.items():
  general_reviews = reviews['general']
  all_words = ' '.join(general_reviews).lower().split()
  word_freq = pd.Series(all_words).value_counts()
  top_words_per_variant[variant] = word_freq.head(10)

for variant, top_words in top_words_per_variant.items():
  print(f"Top 10 words for {variant}:")
  print(top_words)
  print("\n")